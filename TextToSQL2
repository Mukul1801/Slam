import openai
from langchain.agents import create_sql_agent
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from openai import AzureOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
import pandas as pd
import numpy as np
import faiss
import time
import re

HOST = SERVER_NAME+'/'+HOST_HTTP_PATH

db = SQLDatabase.from_databricks(catalog=database, schema=Schema, engine_args={"pool_pre_ping": True},host=HOST,api_token=DATABRICKS_TOKEN,warehouse_id=warehouse_id)

IGNORED_COLUMNS = {
    "#", "Detailed Table Information", "Catalog", "Database", "Table",
    "Created Time", "Last Access", "Created By", "Type", "Location",
    "Provider", "Owner", "Is_managed_location", "Predictive Optimization", "Table Properties"
}

def extract_table_metadata(table_name):
    """Retrieves table metadata and formats it properly."""
    try:
        describe_output = db.run(f"DESCRIBE EXTENDED {table_name}")
        processed_output = eval(describe_output) if isinstance(describe_output, str) else describe_output
    except Exception as e:
        return {"table_name": table_name, "description": f"Error retrieving metadata: {str(e)}", "columns": []}

    column_details = []
    table_description = "No description available"

    for row in processed_output:
        if row[0].strip().lower() == "comment":
            table_description = row[1]
        elif len(row) == 3 and row[0].strip() and row[0].strip() not in IGNORED_COLUMNS:
            column_details.append({
                "name": row[0], 
                "type": row[1], 
                "description": row[2] or "No description available"
            })

    return {
        "table_name": table_name,
        "description": table_description,
        "columns": column_details
    }

def clean_metadata(table_metadata):
    """Clean metadata by removing irrelevant columns."""
    for table in table_metadata:
        table["columns"] = [
            col for col in table["columns"] if col["name"].strip() and col["name"] != "# Detailed Table Information"
        ]
    return table_metadata

# Initialize Azure credentials and clients
scope = "https://cognitiveservices.azure.com/.default"
credential = DefaultAzureCredential()
token_provider = get_bearer_token_provider(credential, scope)

llm_instance = AzureChatOpenAI(
    azure_deployment="gpt-4o-mini",
    api_version=api_version,
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=5,
    verbose=True,
    azure_endpoint=azure_endpoint,
    azure_ad_token_provider=token_provider
)

def get_embeddings(text):
    """Get embeddings for text using Azure OpenAI."""
    client = AzureOpenAI(
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        azure_ad_token_provider=token_provider
    )
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response.data[0].embedding

# Initialize table metadata and FAISS index
print("Loading table metadata...")
tables = db.get_usable_table_names()
table_metadata = [extract_table_metadata(table) for table in tables]
clean_metadata_list = clean_metadata(table_metadata)

# Create FAISS index
table_texts = [
    f"Table: {table['table_name']}\nDescription: {table['description']}\nColumns: {table['columns']}"
    for table in clean_metadata_list
]

print("Creating embeddings and FAISS index...")
embeddings = np.array([get_embeddings(text) for text in table_texts])
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

def query_faiss(query_text, k=3):
    """Uses FAISS to find relevant tables based on query embeddings."""
    query_embedding = np.array(get_embeddings(query_text)).reshape(1, -1).astype("float32")
    distances, indices = index.search(query_embedding, k)
    
    relevant_table_metadata = [clean_metadata_list[i] for i in indices[0]]
    
    formatted_metadata = "\n\n".join(
        f"**Table:** {table['table_name']}\n**Description:** {table['description']}\n**Columns:**\n" +
        "\n".join([f"- {col['name']} ({col['type']}): {col['description']}" for col in table["columns"]])
        for table in relevant_table_metadata
    )
    
    return formatted_metadata

# Define prompt template
SYSTEM_TEMPLATE = """
You are a helpful AI assistant expert in querying SQL Databases to find answers to the user's questions. \
Given an input question, first create a syntactically correct {dialect} SQL query to run, then look at the results of the query and provide a meaningful, human-readable answer to the input question. \
Do not return the raw SQL query result directly as the final answer. Instead, interpret the result and explain it in a concise and clear manner. \
You can order the results by a relevant column to return the most interesting examples in the database. \
Unless otherwise specified, do not return more than {top_k} rows.\
Do not try to pull table information from database and only use the table_info provided.

===Response Guidelines
1. Follow these instructions for creating syntactically correct SQL queries: \
    - Be sure not to query for columns that do not exist in the tables and use alias only where required.\
    - Include unit_name, plant_name, actual_energy, daily_target_volume, and equipment_name in query if mentioned.\
    - Always use the column 'unit_name' associated with the unit in the generated query.\
    - Always use the column 'plant_name' in sql query generation whenever asked for plant or plant Names in user query. \
    - Always use the column 'actual_energy' associated with the energy consumption in the generated query.\
    - Always use the column 'daily_target_volume' associated with the production target, target volumne in the generated query.\
    - Always use the column 'unit_name' associated with the unit name [('Ammonia-Y1',), ('Urea-Y4',), ('Urea-Y2',), ('Mine-8',), ('Urea-Y1',), ('LNG-7',), ('Urea-Y3',), ('LNG-5',), ('Ammonia-Y3',), ('Ammonia-Y2',), ('Ammonia-Y4',), ('LNG-6',)] .\
    - Always use the column 'plant_name' associated with the plant name [('Y2',), ('Y3',), ('Y4',), ('Y1',)]. \
    - Always use the column 'equipment_name' associated with the Asset in the generated query.\
    - Whenever asked for plant or plant Names, return the institute names using column 'plant_name' associated with the 'plant_name' in the generated query.\
    - Likewise, Use appropriate aggregation functions (AVG, SUM). Use'AVG' when average word, 'SUM' when total or overall word is used. Else DO NOT use aggregate functions.\
    - Pay close attention to the filtering criteria mentioned in the question and incorporate them using the WHERE clause in your SQL query.\
    - If the question involves multiple conditions, use logical operators such as AND, OR to combine them effectively.\
    - When dealing with date or timestamp columns, use appropriate date functions (e.g., DATE_PART, EXTRACT) for extracting specific parts of the date or performing date arithmetic.\
    - If the question involves grouping of data (e.g., finding totals or averages for different categories), use the GROUP BY clause along with appropriate aggregate functions.\
    - Consider using aliases for tables and columns to improve readability of the query, especially in case of complex joins or subqueries.\
    - If necessary, use subqueries or common table expressions (CTEs) to break down the problem into smaller, more manageable parts. \
    - To determine the most energy-efficient or energy-inefficient plant, calculate the ratio by dividing the sum of actual production volume by the sum of actual energy used. Display the result and group along with plant namev only and unit if required. Do not apply the HAVING clause in the query.\
    - Incorporate filtering criteria using the WHERE clause.\
    - Use date functions for date or timestamp columns.\

2. After executing the SQL query, interpret the results and provide a meaningful, human-readable answer to the user's question. \
    - For example, if the query returns a numeric value, explain what it represents. \
    - If the query returns multiple rows, summarize the key insights instead of listing all rows. \
    - If the query involves trends or comparisons, describe them clearly.

3. If the provided context is insufficient, explain why the query cannot be generated or why the question cannot be answered.

4. Always format the SQL query for readability before responding.

5. Always respond with a valid, well-formed JSON object in the following format:
{{
    "SQLQuery": "Generated SQL query here",
    "SQLResult": "Raw SQL query result here",
    "Answer": "Human-readable interpretation of the result here"
}}

6. If the user query is about creating graphs or plots, generate a valid SQL query to produce the data required for plotting and explain how the data can be visualized.

7. If you do not know the answer, reply as follows: {{"answer": "I do not know."}}

8. Remove sql keyword from query generated. 

===Response Format
You are required to use the following format, each taking one line:

Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: A detailed, human-readable final answer here
"""

def create_prompt(table_info):
    """Create the prompt template for the agent."""
    return ChatPromptTemplate.from_messages([
        ("system", SYSTEM_TEMPLATE),
        ("human", "Use this table info only: {table_info}"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

# Initialize toolkit
toolkit = SQLDatabaseToolkit(db=db, llm=llm_instance)

def agent_trigger(question):
    """Trigger the agent with a question and return the response."""
    context = query_faiss(question)
    string_prompt = create_prompt(context)

    dbr_agent = create_sql_agent(
        llm=llm_instance,
        toolkit=toolkit,
        prompt=string_prompt,
        agent_type="openai-tools",
        handle_parsing_errors=True,
        verbose=False
    )
    
    human_response = dbr_agent.invoke({
        "dialect": db.dialect,
        "table_info": context,
        "top_k": 3,
        "agent_scratchpad": [],
        "input": question
    })
    return human_response

def parse_response(response_output):
    """Parse the agent response to extract question, SQL query, and answer."""
    try:
        # Try to extract using regex patterns
        question_match = re.search(r"Question:\s*(.*?)\n", response_output, re.DOTALL)
        query_match = re.search(r"SQLQuery:\s*(.*?)\n", response_output, re.DOTALL)
        answer_match = re.search(r"Answer:\s*(.*)", response_output, re.DOTALL)

        question = question_match.group(1).strip() if question_match else "Unknown Question"
        sql_query = query_match.group(1).strip() if query_match else "Unknown Query"
        answer = answer_match.group(1).strip() if answer_match else "No answer available"

        return question, sql_query, answer
    except Exception as e:
        print(f"Error parsing response: {e}")
        return "Parse Error", "Parse Error", "Parse Error"

def test_agent(query):
    """Test the agent with a query and return parsed results with timing."""
    start_time = time.time()
    response = agent_trigger(query)
    end_time = time.time()
    
    response_time = round(end_time - start_time, 2)
    response_output = response["output"].strip()
    
    question, sql_query, answer = parse_response(response_output)
    
    return {
        'Question': query,
        'SQL_Query': sql_query,
        'Answer': answer,
        'Response_Time': response_time
    }

# Test questions
questions = [
    "which plant has the highest actual production in December 2024?",
    "which plant has the lowest actual production in December 2024?",
    "which 3 plants used the most energy in 2024?",
    "What was the most energy efficient plant in 2024?",
    "which asset caused the most lost volume in 2024?",
    "What is the lost volume for gas meter in 2024?",
    "What unit name had the lowest production in 2024?",
    "What unit name had the largest production in 2024?",
    "Which plant had the least production in 2024?",
    "What are the units belonging to Y1 plant?",
    "Which plant type had the largest production in 2024?",
    "Provide the name and energy consumption for the top 3 plants with highest energy consumption in January 2024.",
    "Provide the plant name and total energy consumption for the 3 unit ids with highest energy consumption in January 2024.",
    "For All plants, give a breakdown of total energy consumed for each unit.",
    "For Plants Y1 and Y4, give a breakdown of total energy consumed for each unit.",
    "For All plants, give a breakdown of daily average of energy consumed for each unit.",
    "Which plants have units with average energy consumption less than 500. Provide unit name and daily average consumption.",
    "For LNG-5, which days in January 2024, did we have energy consumption more than 250.",
    "Which days did LNG-5 not meet target volume?",
    "Which days did LNG-5 use more energy than its average energy consumption",
    "Which days did LNG-5 use more energy than its average energy consumption and also not meet its production target?",
    "which plants are inefficient?",
    "Which day was the worst in terms of energy usage and production?"
]

def run_tests_and_save_excel():
    """Run all tests and save results to Excel file."""
    print("Starting agent tests...")
    results = []
    
    for idx, question in enumerate(questions, start=1):
        print(f"Processing question {idx}/{len(questions)}: {question[:50]}...")
        try:
            result = test_agent(question)
            results.append(result)
        except Exception as e:
            print(f"Error processing question {idx}: {e}")
            results.append({
                'Question': question,
                'SQL_Query': f"Error: {str(e)}",
                'Answer': f"Error: {str(e)}",
                'Response_Time': 0
            })
    
    # Create DataFrame and save to Excel
    df = pd.DataFrame(results)
    output_file = "agent_responses.xlsx"
    
    try:
        df.to_excel(output_file, index=False, engine='openpyxl')
        print(f"Results saved to {output_file}")
        
        # Print summary
        print(f"\nSummary:")
        print(f"Total questions processed: {len(results)}")
        print(f"Average response time: {df['Response_Time'].mean():.2f} seconds")
        print(f"Max response time: {df['Response_Time'].max():.2f} seconds")
        print(f"Min response time: {df['Response_Time'].min():.2f} seconds")
        
    except Exception as e:
        print(f"Error saving to Excel: {e}")
        # Fallback to CSV
        csv_file = "agent_responses.csv"
        df.to_csv(csv_file, index=False)
        print(f"Results saved to {csv_file} instead")

if __name__ == "__main__":
    run_tests_and_save_excel()
