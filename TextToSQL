import openai
import json
import pickle
import time
import pandas as pd
from pathlib import Path
from typing import Dict, List
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

from langchain.agents import create_sql_agent
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from langchain_openai import AzureChatOpenAI
from openai import AzureOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

import numpy as np
import faiss

# Configuration - Update with your values
SERVER_NAME = "your-databricks-server"
HOST_HTTP_PATH = "your-http-path"
DATABASE = "your_database"
WAREHOUSE_ID = "your_warehouse_id"
DATABRICKS_TOKEN = "your_token"
AZURE_ENDPOINT = "https://your-resource.openai.azure.com/"
API_VERSION = "2024-02-15-preview"
AZURE_SCOPE = "https://cognitiveservices.azure.com/.default"

class TextToSQLBot:
    def __init__(self):
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.results = []
        
        # Initialize connections
        self.credential = DefaultAzureCredential()
        self.token_provider = get_bearer_token_provider(self.credential, AZURE_SCOPE)
        
        # Database setup
        HOST = f"{SERVER_NAME}/{HOST_HTTP_PATH}"
        self.db = SQLDatabase.from_databricks(
            catalog=DATABASE, 
            schema="abc", 
            engine_args={"pool_pre_ping": True},
            host=HOST,
            api_token=DATABRICKS_TOKEN,
            warehouse_id=WAREHOUSE_ID
        )
        
        # LLM setup
        self.llm = AzureChatOpenAI(
            azure_deployment="gpt-4o-mini",
            api_version=API_VERSION,
            temperature=0,
            azure_endpoint=AZURE_ENDPOINT,
            azure_ad_token_provider=self.token_provider
        )
        
        # Embedding client
        self.embedding_client = AzureOpenAI(
            api_version=API_VERSION,
            azure_endpoint=AZURE_ENDPOINT,
            azure_ad_token_provider=self.token_provider
        )
        
        self.table_metadata = []
        self.faiss_index = None
        
    def get_table_metadata(self):
        """Extract and cache table metadata."""
        cache_file = self.cache_dir / "metadata.pkl"
        
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        
        tables = self.db.get_usable_table_names()
        metadata = []
        
        def extract_metadata(table_name):
            try:
                describe_output = self.db.run(f"DESCRIBE EXTENDED {table_name}")
                processed_output = eval(describe_output) if isinstance(describe_output, str) else describe_output
                
                columns = []
                description = "No description available"
                
                for row in processed_output:
                    if len(row) >= 2 and row[0].strip().lower() == "comment":
                        description = row[1]
                    elif len(row) == 3 and row[0].strip():
                        columns.append({
                            "name": row[0].strip(),
                            "type": row[1].strip(),
                            "description": (row[2] or "").strip()
                        })
                
                return {
                    "table_name": table_name,
                    "description": description,
                    "columns": columns
                }
            except Exception:
                return {
                    "table_name": table_name,
                    "description": "Error retrieving metadata",
                    "columns": []
                }
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            metadata = list(executor.map(extract_metadata, tables))
        
        # Cache results
        with open(cache_file, 'wb') as f:
            pickle.dump(metadata, f)
        
        return metadata
    
    @lru_cache(maxsize=1000)
    def get_embeddings(self, text: str):
        """Get embeddings with caching."""
        response = self.embedding_client.embeddings.create(
            model="text-embedding-ada-002",
            input=text
        )
        return np.array(response.data[0].embedding, dtype=np.float32)
    
    def build_vector_index(self):
        """Build FAISS index for table search."""
        index_file = self.cache_dir / "faiss_index.pkl"
        
        if index_file.exists():
            with open(index_file, 'rb') as f:
                self.faiss_index, self.table_metadata = pickle.load(f)
            return
        
        self.table_metadata = self.get_table_metadata()
        
        # Create text representations
        table_texts = []
        for table in self.table_metadata:
            text = f"Table: {table['table_name']} | Description: {table['description']} | "
            text += f"Columns: {', '.join([f\"{col['name']} ({col['type']})\" for col in table['columns']])}"
            table_texts.append(text)
        
        # Generate embeddings
        embeddings = [self.get_embeddings(text) for text in table_texts]
        embeddings_array = np.array(embeddings, dtype=np.float32)
        
        # Build FAISS index
        dimension = embeddings_array.shape[1]
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(embeddings_array)
        
        # Cache index
        with open(index_file, 'wb') as f:
            pickle.dump((self.faiss_index, self.table_metadata), f)
    
    def find_relevant_tables(self, query: str, k=3):
        """Find top-k relevant tables for query."""
        query_embedding = self.get_embeddings(query).reshape(1, -1)
        distances, indices = self.faiss_index.search(query_embedding, k)
        
        relevant_tables = [self.table_metadata[i] for i in indices[0]]
        table_names = [table['table_name'] for table in relevant_tables]
        
        # Format context
        context = ""
        for table in relevant_tables:
            context += f"**Table: {table['table_name']}**\n"
            context += f"Description: {table['description']}\n"
            context += "Columns:\n"
            for col in table['columns']:
                context += f"- {col['name']} ({col['type']}): {col['description']}\n"
            context += "\n"
        
        return context, table_names
    
    def create_prompt(self, table_context):
        """Create SQL generation prompt."""
        system_template = """You are an expert SQL query generator. Generate accurate SQL queries based on user questions.

INSTRUCTIONS:
1. Use ONLY the table schema provided below
2. Create syntactically correct SQL queries
3. Return response in JSON format: {{"SQLQuery": "query", "SQLResult": "result", "Answer": "explanation"}}
4. Use appropriate column names: unit_name, plant_name, actual_energy, daily_target_volume, equipment_name

Available Tables:
{table_info}"""

        return ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
    
    def process_query(self, user_query: str):
        """Process a single query and return results."""
        start_time = time.time()
        
        try:
            # Find relevant tables
            table_context, relevant_tables = self.find_relevant_tables(user_query)
            
            # Create agent
            prompt = self.create_prompt(table_context)
            toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)
            
            agent = create_sql_agent(
                llm=self.llm,
                toolkit=toolkit,
                prompt=prompt,
                agent_type="openai-tools",
                handle_parsing_errors=True,
                verbose=False
            )
            
            # Execute query
            response = agent.invoke({
                "table_info": table_context,
                "input": user_query,
                "agent_scratchpad": []
            })
            
            # Extract information
            output = response.get('output', '')
            sql_query = self.extract_sql_query(output)
            
            runtime = time.time() - start_time
            
            # Store result
            result = {
                'user_query': user_query,
                'relevant_tables': ', '.join(relevant_tables),
                'sql_query': sql_query,
                'agent_answer': output,
                'runtime_seconds': round(runtime, 2)
            }
            
            self.results.append(result)
            return result
            
        except Exception as e:
            runtime = time.time() - start_time
            result = {
                'user_query': user_query,
                'relevant_tables': '',
                'sql_query': f'Error: {str(e)}',
                'agent_answer': f'Error processing query: {str(e)}',
                'runtime_seconds': round(runtime, 2)
            }
            self.results.append(result)
            return result
    
    def extract_sql_query(self, text):
        """Extract SQL query from agent response."""
        try:
            # Try to parse JSON response
            if "SQLQuery" in text:
                start = text.find('{"')
                if start != -1:
                    brace_count = 0
                    for i, char in enumerate(text[start:], start):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_str = text[start:i+1]
                                parsed = json.loads(json_str)
                                return parsed.get("SQLQuery", "")
        except:
            pass
        
        # Fallback: find SELECT statement
        lines = text.split('\n')
        for line in lines:
            if line.strip().upper().startswith('SELECT'):
                return line.strip()
        
        return "SQL query not found"
    
    def export_to_excel(self, filename=None):
        """Export results to Excel."""
        if not self.results:
            return "No results to export"
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"text_to_sql_results_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.results)
        df.columns = ['User Query', 'Relevant Tables', 'SQL Query Generated', 'Agent Answer', 'Runtime (seconds)']
        
        # Save to Excel
        filepath = Path(filename)
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Results', index=False)
            
            # Adjust column widths
            worksheet = writer.sheets['Results']
            worksheet.column_dimensions['A'].width = 50  # User Query
            worksheet.column_dimensions['B'].width = 30  # Relevant Tables
            worksheet.column_dimensions['C'].width = 80  # SQL Query
            worksheet.column_dimensions['D'].width = 60  # Agent Answer
            worksheet.column_dimensions['E'].width = 15  # Runtime
        
        return str(filepath)

def main():
    """Run the text-to-SQL system."""
    # Test queries
    queries = [
        "What is the average energy consumption by plant?",
        "Which unit has the highest production target?",
        "Show me the energy efficiency ratio for each plant",
        "What equipment is used in the Y1 plant?",
        "Compare energy consumption between plants"
    ]
    
    # Initialize bot
    bot = TextToSQLBot()
    
    print("Initializing system...")
    bot.build_vector_index()
    
    print(f"Processing {len(queries)} queries...")
    
    # Process each query
    for i, query in enumerate(queries, 1):
        print(f"[{i}/{len(queries)}] {query}")
        result = bot.process_query(query)
        print(f"Runtime: {result['runtime_seconds']}s")
    
    # Export results
    excel_file = bot.export_to_excel()
    print(f"\nResults exported to: {excel_file}")
    
    return bot

if __name__ == "__main__":
    bot = main()
